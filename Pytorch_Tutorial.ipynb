{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# PyTorch Introduction\n",
    "\n",
    "Goal takeways:\n",
    "- Automatic differentiation is a powerful tool\n",
    "- PyTorch implements common function used in deep learning\n",
    "- Data Processing with PyTorch DataSet\n",
    "- Mixed Presision Training in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(446)\n",
    "np.random.seed(446)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tensors and relation to numpy\n",
    "\n",
    "By this point, we have worked with numpy quite a bit. PyTorch's building block, the `tensor` is similar to numpy's `ndarray`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_numpy, x_torch\n",
      "[0.1 0.2 0.3] tensor([0.1000, 0.2000, 0.3000])\n",
      "\n",
      "to and from numpy and pytorch\n",
      "tensor([0.1000, 0.2000, 0.3000], dtype=torch.float64) [0.1 0.2 0.3]\n",
      "\n",
      "x + y\n",
      "[3.1 4.2 5.3] tensor([3.1000, 4.2000, 5.3000])\n",
      "\n",
      "norm\n",
      "0.37416573867739417 tensor(0.3742)\n",
      "\n",
      "mean along the 0th dimension\n",
      "[2. 3.] tensor([2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# we create tensors in a similar way to numpy nd arrays\n",
    "x_numpy = np.array([0.1, 0.2, 0.3])\n",
    "x_torch = torch.tensor([0.1, 0.2, 0.3])\n",
    "print('x_numpy, x_torch')\n",
    "print(x_numpy, x_torch)\n",
    "print()\n",
    "\n",
    "# to and from numpy, pytorch\n",
    "print('to and from numpy and pytorch')\n",
    "print(torch.from_numpy(x_numpy), x_torch.numpy())\n",
    "print()\n",
    "\n",
    "# we can do basic operations like +-*/\n",
    "y_numpy = np.array([3, 4, 5.])\n",
    "y_torch = torch.tensor([3, 4, 5])\n",
    "print('x + y')\n",
    "print(x_numpy + y_numpy, x_torch + y_torch)\n",
    "print()\n",
    "\n",
    "# many functions that are in numpy are also in pytorch\n",
    "print(\"norm\")\n",
    "print(np.linalg.norm(x_numpy), torch.norm(x_torch))\n",
    "print()\n",
    "\n",
    "# to apply an operation along a dimension,\n",
    "# we use the dim keyword argument instead of axis\n",
    "print('mean along the 0th dimension')\n",
    "x_numpy = np.array([[1, 2.], [3, 4.]])\n",
    "x_torch = torch.tensor([[1, 2.], [3, 4.]])\n",
    "print(np.mean(x_numpy, axis=0), torch.mean(x_torch, dim=0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `Tensor.view`\n",
    "We can use the `Tensor.view()` function to reshape tensors similarly to `numpy.reshape()`\n",
    "\n",
    "It can also automatically calculate the correct dimension of `a - 1` is passed in. This is useful if we are working with\n",
    "batches, but the batch size is unknown."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 3, 28, 28])\n",
      "torch.Size([10000, 3, 784])\n",
      "torch.Size([10000, 3, 784])\n"
     ]
    }
   ],
   "source": [
    "# MNIST\n",
    "N, C, W, H = 10000, 3, 28, 28\n",
    "X = torch.randn((N, C, W, H))\n",
    "\n",
    "print(X.shape)\n",
    "print(X.view(N, C, 784).shape)\n",
    "print(X.view(-1, C, 784).shape)  # automatically choose the 0th dimension"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Broadcasting Semantics\n",
    "Two tensors are \"Broadcastable\" if the following rules hold:\n",
    "- Each tensor has at least one dimension\n",
    "- When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal,\n",
    "one of them is 1, or one of them does not exist."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "# PyTorch operations support NumPy Broadcasting Semantics\n",
    "x = torch.empty(5, 1, 4, 1)\n",
    "y = torch.empty(3, 1, 1)\n",
    "print((x + y).size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computation graphs\n",
    "\n",
    "What's special about PyTorch's `tensor` object is that it implicitly creates a computation graph in the background. A\n",
    "computation graph is a way of writing a mathematical expression as a graph. There is an algorithm to compute the gradients\n",
    "of all the variable of a computation graph in time on the same order it is to compute the function itself.\n",
    "\n",
    "Consider the expression $e = (a+b) * (b+1)$ with values $a=2, b=1$. We can draw the evaluated computation graph as in\n",
    "PyTorch, we can write this as\n",
    "\n",
    "[![](https://mermaid.ink/img/eyJjb2RlIjoiZ3JhcGggVERcbiAgICBFKGUgPSBjKmQsZT02KVxuICAgIEMoYyA9IGErYiwgYyA9IDMpXG4gICAgRChkID0gYisxLCBkID0gMilcbiAgICBBKGEsIGE9MilcbiAgICBCKGIsIGI9MSlcbiAgICBBIC0tPiBDXG4gICAgQiAtLT4gQ1xuICAgIEMgLS0-IEVcbiAgICBEIC0tPiBFIiwibWVybWFpZCI6eyJ0aGVtZSI6ImRlZmF1bHQifSwidXBkYXRlRWRpdG9yIjpmYWxzZSwiYXV0b1N5bmMiOnRydWUsInVwZGF0ZURpYWdyYW0iOmZhbHNlfQ)](https://mermaid-js.github.io/mermaid-live-editor/edit#eyJjb2RlIjoiZ3JhcGggVERcbiAgICBFKGUgPSBjKmQsZT02KVxuICAgIEMoYyA9IGErYiwgYyA9IDMpXG4gICAgRChkID0gYisxLCBkID0gMilcbiAgICBBKGEsIGE9MilcbiAgICBCKGIsIGI9MSlcbiAgICBBIC0tPiBDXG4gICAgQiAtLT4gQ1xuICAgIEMgLS0-IEVcbiAgICBEIC0tPiBFIiwibWVybWFpZCI6IntcbiAgXCJ0aGVtZVwiOiBcImRlZmF1bHRcIlxufSIsInVwZGF0ZUVkaXRvciI6ZmFsc2UsImF1dG9TeW5jIjp0cnVlLCJ1cGRhdGVEaWFncmFtIjpmYWxzZX0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "2.0\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(2.0, requires_grad=True)  # we set requires_grad=True to let PyTorch know to keep the graph\n",
    "b = torch.tensor(1.0, requires_grad=True)\n",
    "c = a + b\n",
    "d = b + 1\n",
    "e = c * d\n",
    "print(f'{c}')\n",
    "print(f'{d}')\n",
    "print(f'{e}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that PyTorch kept track of the computation graph for us."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CUDA Semantics\n",
    "\n",
    "It's easy copy tensor from cpu to gpu or from gpu to cpu."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3959, 0.6177, 0.7256, 0.0971, 0.9186, 0.8277, 0.4409, 0.9344, 0.8967,\n",
      "        0.1897])\n",
      "tensor([0.3959, 0.6177, 0.7256, 0.0971, 0.9186, 0.8277, 0.4409, 0.9344, 0.8967,\n",
      "        0.1897], device='cuda:0')\n",
      "tensor([0.3959, 0.6177, 0.7256, 0.0971, 0.9186, 0.8277, 0.4409, 0.9344, 0.8967,\n",
      "        0.1897])\n"
     ]
    }
   ],
   "source": [
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "# on Colab we should set torch.device(\"gpu\"), on personal computer with single GPU we set cuda\n",
    "gpu = torch.device(\"cuda\")\n",
    "\n",
    "x = torch.rand(10)\n",
    "print(x)\n",
    "x = x.to(gpu)\n",
    "print(x)\n",
    "x = x.to(cpu)\n",
    "print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PyTorch as an auto grad framework\n",
    "\n",
    "Now that we have seen PyTorch keeps the graph around for us, let's use it to compute some gradients for us.\n",
    "\n",
    "Consider the function $f(x) = (x-2)^2$\n",
    "\n",
    "Q: Compute $\\frac{df(x)}{dx}$ and then compute $f'(1)$\n",
    "\n",
    "We make a backward() call on the leaf variable(y) in the computation, computing all the gradients of `y` at once."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical f'(x): tensor([-2.], grad_fn=<MulBackward0>)\n",
      "PyTorch's f'(x): tensor([-2.])\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return (x - 2) ** 2\n",
    "\n",
    "\n",
    "def fp(x):\n",
    "    return 2 * (x - 2)\n",
    "\n",
    "\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "y = f(x)\n",
    "y.backward()\n",
    "\n",
    "print(f'Analytical f\\'(x): {fp(x)}')\n",
    "print(f'PyTorch\\'s f\\'(x): {x.grad}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It can also find gradients of functions.\n",
    "\n",
    "Let $\\omega = [\\omega_1, \\omega_2]^T$\n",
    "\n",
    "Consider $g(\\omega) = 2\\omega_1\\omega_2 + \\omega_2 \\cos(\\omega_1)$\n",
    "\n",
    "Q: Compute $\\nabla_{\\omega}g(\\omega) and verify \\nabla_{\\omega}g([\\pi, 1]) = [2, \\pi - 1]^T$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical grad g(w): tensor([2.0000, 5.2832])\n",
      "Pytorch\\s grad(w): tensor([2.0000, 5.2832])\n"
     ]
    }
   ],
   "source": [
    "def g(w):\n",
    "    return 2 * w[0] * w[1] + w[1] * torch.cos(w[0])\n",
    "\n",
    "\n",
    "def grad_g(w):\n",
    "    return torch.tensor([2 * w[1] - w[1] * torch.sin(w[0]), 2 * w[0] + torch.cos(w[0])])\n",
    "\n",
    "w = torch.tensor([np.pi, 1], requires_grad=True)\n",
    "\n",
    "z = g(w)\n",
    "z.backward()\n",
    "\n",
    "print(f'Analytical grad g(w): {grad_g(w)}')\n",
    "print(f'Pytorch\\s grad(w): {w.grad}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using the gradients\n",
    "\n",
    "Now that we have gradients, we can use our favorite optimization algorithm, gradient descent!\n",
    "\n",
    "Let $f% the same function we defined above.\n",
    "\n",
    "Q: What is the value of $x$ that minimizes $f$?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the result: 5.000\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.0], requires_grad=True)\n",
    "step_size = 0.25\n",
    "\n",
    "print(f'This is the result: {x.item():.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter, \tx, \tf(x), \tf'(x), \tf'(x) pytorch\n",
      "0 \t5.000 \t9.000 \t6.000 \t6.000\n",
      "1 \t3.500 \t2.250 \t3.000 \t3.000\n",
      "2 \t2.750 \t0.562 \t1.500 \t1.500\n",
      "3 \t2.375 \t0.141 \t0.750 \t0.750\n",
      "4 \t2.188 \t0.035 \t0.375 \t0.375\n",
      "5 \t2.094 \t0.009 \t0.188 \t0.188\n",
      "6 \t2.047 \t0.002 \t0.094 \t0.094\n",
      "7 \t2.023 \t0.001 \t0.047 \t0.047\n",
      "8 \t2.012 \t0.000 \t0.023 \t0.023\n",
      "9 \t2.006 \t0.000 \t0.012 \t0.012\n",
      "10 \t2.003 \t0.000 \t0.006 \t0.006\n",
      "11 \t2.001 \t0.000 \t0.003 \t0.003\n",
      "12 \t2.001 \t0.000 \t0.001 \t0.001\n",
      "13 \t2.000 \t0.000 \t0.001 \t0.001\n",
      "14 \t2.000 \t0.000 \t0.000 \t0.000\n"
     ]
    }
   ],
   "source": [
    "print('iter, \\tx, \\tf(x), \\tf\\'(x), \\tf\\'(x) pytorch')\n",
    "for i in range(15):\n",
    "    y = f(x)\n",
    "    y.backward()  # compute the gradient\n",
    "\n",
    "    # print(\"{}, \\t{:.3f}, \\t{:.3f}, \\t{:.3f}, \\t{:.3f}\".format(i, x.item(), f(x).item(), fp(x).item, x.grad.item()))\n",
    "    print(f'{i} \\t{x.item():.3f} \\t{f(x).item():.3f} \\t{fp(x).item():.3f} \\t{x.grad.item():.3f}')\n",
    "    x.data = x.data - step_size * x.grad  # perform a GD update step\n",
    "\n",
    "    # We need to zero the grad variable since the backward()\n",
    "    # call accumulates the gradients in .grad instead of overwriting.\n",
    "    # The detach_() is for efficiency. You do not need to worry to much about it.\n",
    "    x.grad.detach_()\n",
    "    x.grad.zero_()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}